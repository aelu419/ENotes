# Materials
- shader + parameters = material
  - textures, numbers, etc.
- texture settings
  - sRGB: square root mapping that needs to be squared and square-rooted within the rendering pipeline
    - enabling the feature allows the GPU to handle this process via hardware
    - this is usually only applied to color textures, for normal maps and etc. it is unnecessary

## Material Extraction
- newer version of Unity usually keep imported materials within the mesh object
  - embedded materials cannot be edited
- to extract, choose the mesh object and go to Materials -> Extract Materials
  - to use extracted materials, go to mesh and select Materials -> Location -> Use extermal materials (Legacy)
- height maps can be converted to normal maps by    
  - importing the texture as a normal map
  - use "create from grayscale" setting to convert grayscale heightmap to rgb normal map

# Unity's Render Pipelines
## Built-in Pipelines
- forward rendering: basic rasterization model
- deferred rendering: forward rendering but with precomputed lighting and specialized shaders to fit them
- certain legacy piplines, rarely used
## Scriptable Render Pipelines
- URP (i.e. LWRP)
  - exposes functionality of the built-in pipeline
  - can be scaled up/down depending on platform
- HDRP
- Custom Pipelines
  - note that Unity open sources scriptable render pipeline components

# Demo Shaders
- between `BEGIN HLSL` and `END HLSL`, surrounded by Unity patterns like subshaders, passes, etc.
- `#include "UnityCG.cginc"` to incorporate built-in definitions
  - the same variables may not be supported by other platforms
  - matrix naming like `unity_MatrixVP` is in the order of *application* of transforms, not their matrix multiplicatin order
    - the actual matrix is $PV$
- `#pragma` informs the compiler
  - `#pragma vertex VertSolidColor` means that there is a function called `VertSolidColor` defined below, which is selected as the vertex shader
  - `#pragma fragment FragSolidColor` means that there is a function called `FragSolidColor` defined below, selected as the fragment shader
- in function parameters, *semantics* can be specified
  - `float4 VertSolidColor(float4 positionOS:POSITION) : SV_POSITION { ... }`
  - `POSITION` will stream into the variable `positionOS`
  - `SV_POSITION` is where the output writes to
- alternatively,
  - `void VertSolidColorOut(float4 v:POSITION, out sv:SV_POSITION)`
    - write to `sv` within the function

## Matrices
- `Matrix4x4` class and its functions for initializing matrices in Unity (C#, not HLSL)
  - `Matrix4x4.ortho(...)`, `Matrix4x4.perspective(...)` initializes projection matrices from parameters
- builtin matrices have complications with negative signs and coefficients (/2 or not)

## Simple Vertex Shader
- ``` HLSL
  // injecting a w value and transform to world space
  float4 positionWS = mul(unity_ObjectToWorld, float4(positionOS.xyz, 1.0));
  // transform to clip space
  sv = mul(unity_MatrixVP, positionWS);
  ```

## Vertex Shader with Texture Coordinates
- ``` HLSL

  sampler2D _MainTex;
  // texture-specific coordinate, with sampler settings like tiling and offset
  // this is cretaed by Unity with the convention of <texture name>_ST
  float4 _MainTex_ST;

  void VertTextured(
    float4 pOS:POSITION,
    float2 uv_in: TEXCOORD0,
    out float4 sv:SV_POSITION,
    out float2 uv_out: TEXCOORD0) {

    // ... omitted position transforms

    // the issue with the below statement is that it ignores the texture offset and tiling settings
    uv_out = uv_in; // replace this direct assignment to achieve texture effects

    // this accommodates for tiling and offset, but just for the main texture
    // TRANSFORM_TEX is actually a macro, not a function per se
    uv_out = TRANSFORM_TEX(uv, _MainTex);
  }

  float4 FragTextured(float2 uv:TEXCOORD0): COLOR {
    // the issue of using this below is the texture doesn't repond to tiling and offset settings
    return tex2D(_MainTex, uv);
  }
  ```
- note that most engines support texture sampling in the vertex shader as well

## Structs
- `struct` in HLSL can contain semantics
- *ex.* 
  - ``` HLSL
    // appdata to vertex shader
    struct a2v {
        float4 p: POSITION;
        float2 uv: TEXCOORD0;
    };

    // vertex shader to fragment shader
    struct v2f {
        float4 sv: SV_POSITION;
        float2 uv: TEXCOORD0;
    }

    v2f VertTexturedStruct(a2v input) { ... }

    float4 FragTexturedStruct(v2f input) : COLOR { ... }
    ```
- this is just stylistics

## Normal Transforms
- effect of uneven scaling on normal vectors
![](../../res/ECE4795/22.1.png)
  - in this case, simply multiplying the matrix does not apply the correct transform
- derivation of the normal transform
  - tangent $t$, normal vector $n$ should remain orthogonal
  - for some matrix $W$ such that $Wn \perp Mt$ (i.e. $Wn \cdot Mt = 0$)
  - $(Wn)^tMt=0$ (rewrite dot product as regular matrix multiplication)
  - $(n^tW^t)Mt=0$ (expanding the transpose)
  - $n^t(W^tM)t = 0$
  - setting $W^tM =I$, $W=M^{-T}$, this is the transform to apply to $n$ such that it will remain orthogonal against any tangent $t$
  - note that for a purely rotational (i.e. orthonormal) matrix, $W=M$ because transpose is the same with inverse, but in general $W$ should be kept as $M^{-T}
- in the context of Unity shading, this is done by
  - `float3 normalWS = normalize(mul(input.normalOS, (float3x3) unity_WorldToObject));`
  - note that premultiplication handles the transformed part $Wn=n^TW^T=n^TM^{-1}$
  - $M^{-1}$ is done by using WorldToObject instead of ObjectToWorld

## Vertex-Lighting Shaders
- setup
  - shader properties
    ``` hlsl
    _BaseTex("Base (RGB) Gloss (A)", 2D) = "white" {}
    _Attenuation("Attenuation Factor", float) = 1
    ```
  - shader tags
    - for point light: `Tags { "LightMode" = "ForwardAdd" }`
    - for directional light: `Tags { "LightMode" = "ForwardBase" }`
    - different tags denote different parts of the Unity shader code that is run at different passes, using the tags enable the shader to use those passes' results
  - inputs
    - ``` hlsl
      sampler2D  _BaseTex;
      float4 _BaseTex_ST;
      float _Attenuation;
      float4 _LightColor0; // filled by Unity
      ```
  - structs
    - ``` hlsl
      struct a2v {
        float4 positionOS: POSITION;
        float4 normalOS: NORMAL;
        float2 uv: TEXCOORD0;
      };
      ```
    - ``` hlsl
      struct v2f {
        float4 sv: SV_POSITION;
        float2 uv: TEXCOORD0;
        float3 diffAlmost: TEXCOORD1 // uses a texture coordinate to transfer non-coordinate data, namely lighting
      };
      ```
- in the vertex shader
``` hlsl
// in addition to transforming position, texcoord, and normal

// the convention is that w=0 is directional light, and w=1 is point light
// for directional light, light position is by itself the direction
float3 rawLightDir = _WorldSpaceLightPos0.xyz - positionWS.xyz * _WorldSpaceLightPos0.w;
float3 lightDir = normalize(rawLightDir);
float l2 = dot(rawLightDir, rawLightDir);
float attenuation = 1.0 / (1.0 + l2 * _Attenuation * _WorldSpaceLightPos0.w); // again, only point light is attenuated

// write light color to TEXCOORD1
// diff stands for diffuse lighting
output.diffAlmost = _LightColor0.rgb * max(0, dot(normalWS, lightDir)) * attenuation;

```
- in the fragment shader
``` hlsl
float3 base = tex2D(_BaseTex, input.uv);
return (float4(input.diffAlmost * base.rgb, 1));
```

## Per-Pixel Lighting Shader
- setup
  - use `TEXCOORD1` for `positionWS`, `TEXCOORD2` for `normalWS`
  - other settings remain the same
- vertex shader
  - write `positionWS` instead of light color to `TEXCOORD1`
  - keep `normalWS` and write it to `TEXCOORD2` instead of just using it to get light color
- note the `positionWS` and `normalWS` will get interpolated by the GPU once it gets to the fragment shader
- fragment shader
``` hlsl
// ... do the same calculations for lightDir, l2, diffAlmost, and base

// note that normalWS needs to be re-normalized after interpolation
// diffAlmost computation will take the re-normalized normalWS instead of using it directly

float3 output = diffAlmost * base.rgb;

return float4(output, 1);
```

## Normal Mapping
- in addition to the pixel lighting shader
- setup
  - `_NormalMap("Normal Map", 2D) = "bump" {}`
- inputs
  - `sampler2D _NormalMap;`
  - `float4 _NormalMap_ST;`
- structures
  - ```
    struct a2v {
      float4 positionOS: POSITION;
      float3 normalOS: NORMAL;
      float4 tangentOS: TANGENT;
      float2 uv: TEXCOORD0;
    }
    ```
  - ```
    struct v2f {
      float4 sv: SV_POSITION;
      float2 bmap_uv: TEXCOORD0; // corresponding to _BaseTex
      float2 nmap_uv: TEXCOORD1; // corresponding to _NormalMap
      float3 positionWS: TEXCOORD2;
      float3 tangentWS: TEXCOORD4;
      float3 normalWS: TEXCOORD3;
      float3 bitangentWS: TEXCOORD5;
    }
    ```
- in the vertex shader
  - tangent transformation is just applying the world transform matrix, instead of the same complication with normal transform
  - the bitangent is computed manually in the vertex shader
    - `output.bitangentWS = normalize(cross(output.normalWS, output.tangentWS) * input.tangentOS.w)`
    - this completes the R3 basis
    - note that the `w` channel of `TANGENT` is actually used to encode the handedness of the bitangent
      - the `TANGENT` channel is not actually a homogeneous direction vector with $w=0$
      - Unity ditches whatever bitangent support the hardware might have
  - both uv's are computed via `TRANSFORM_TEX(input.uv, <base texture name>)`
    - the macro handles different tiling, and offset settings per texture
- in the fragment shader
```
// due to designs of popular compressions, G and A channels preserve more precision
// for this reason, Unity packs X and Y in A and G
// also, normal maps use the range [0, 1] instead of [-1, 1], which needs to be converted back
float2 nXY = 2 * tex2D(_NormalMap, input.nmap_uv).ag - 1;
// normal vectors are unit vectors
float nZ = sqrt(1 - saturate(dot(nXY, nXY)));

// renormalize world space vectors after interpolation
float3 nWS = normalize(input.normalWS);
float3 tWS = normalize(input.tangentWS);
float3 btWS = normalize(intput.bitangentWS);

// normal map perturbs the normal vector in the tangent space, determined by the n, t, bt basis
// effectively this does [tWS, btWS, nWS][nXY.x, nXY.y, nZ]^T
float3 newNormal = tWS * nXY.x + btWS * nXY.y + nWS * nZ;
newNormal = normalize(newNormal);

// ... the rest is basically the pixel shader, except the calculation of diffAlmost uses the newNormal
```
## Note for Directional Lighting
- the previous shaders all use the `ForwardAdd` pass, which is reserved for point lights as well as the directional lights that Unity deems unimportant
- for this reason, when only directional lighting is used, the fragments won't return anything because they are not being called
- to account for both point and directional lights, write separate `subshader`s for each lightmode tag